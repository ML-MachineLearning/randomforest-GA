{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import os\n",
    "import glob\n",
    "import datacube\n",
    "import itertools\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import fiona\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import shape\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Set up datacube instance\n",
    "dc = datacube.Datacube(app = 'Random forest classification')\n",
    "\n",
    "def rasterize_vector(input_data, cols, rows, geo_transform,\n",
    "                     projection, field):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rasterize a vector file and return numpy array\n",
    "    :attr vector_data_path: input shapefile path\n",
    "    :attr cols: width of output array in columns\n",
    "    :attr rows: height of output array in rows\n",
    "    :attr geo_transform: geotransform for rasterization\n",
    "    :attr projection: projection for rasterization\n",
    "    :attr field: shapefile field to take values from\n",
    "    :returns: a 'row x col' array containg values from vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(input_data, str):    \n",
    "    \n",
    "        # Open vector with gdal\n",
    "        data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "        input_data = data_source.GetLayer(0)\n",
    "    \n",
    "    # Set up output raster\n",
    "    driver = gdal.GetDriverByName('MEM')  # In memory dataset\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    \n",
    "    # Rasterize shapefile and extract array\n",
    "    gdal.RasterizeLayer(target_ds, [1], input_data, options=[\"ATTRIBUTE=\" + field])\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    out_array = band.ReadAsArray()\n",
    "    target_ds = None\n",
    "    \n",
    "    return out_array\n",
    "\n",
    "\n",
    "def write_geotiff(fname, data, geo_transform, projection, nodata_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a single band GeoTIFF file with data from array.\n",
    "    :attr fname: output file path\n",
    "    :attr data: input array\n",
    "    :attr geo_transform: geotransform for output raster\n",
    "    :attr projection: projection for output raster\n",
    "    :attr nodata_val: value to convert to nodata in output raster\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # Create raster of given size and projection\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    \n",
    "    # Write data to array and set nodata values\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    band.SetNoDataValue(nodata_val)\n",
    "    \n",
    "    # Close file\n",
    "    dataset = None  \n",
    "    \n",
    "    \n",
    "# def write_multibandgeotiff(filename, dataset, time_index=None, profile_override=None):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Write an xarray dataset to a geotiff\n",
    "#     :attr bands: ordered list of dataset names\n",
    "#     :attr time_index: time index to write to file\n",
    "#     :attr dataset: xarray dataset containing multiple bands to write to file\n",
    "#     :attr profile_override: option dict, overrides rasterio file creation options.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Geotiff settings for export\n",
    "#     DEFAULT_PROFILE = {\n",
    "#         'blockxsize': 128,  # changed from 256\n",
    "#         'blockysize': 128,  # changed from 256 \n",
    "#         'compress': 'lzw',\n",
    "#         'driver': 'GTiff',\n",
    "#         'interleave': 'band',\n",
    "#         'nodata': 9999,\n",
    "#         'tiled': True}\n",
    "    \n",
    "#     profile_override = profile_override or {}\n",
    "\n",
    "#     dtypes = {val.dtype for val in dataset.data_vars.values()}\n",
    "#     assert len(dtypes) == 1  # Check for multiple dtypes\n",
    "#     profile = DEFAULT_PROFILE.copy()\n",
    "#     profile.update({'width': dataset.dims[dataset.crs.dimensions[1]],\n",
    "#                     'height': dataset.dims[dataset.crs.dimensions[0]],\n",
    "#                     'transform': dataset.affine,\n",
    "#                     'crs': dataset.crs.crs_str,\n",
    "#                     'count': len(dataset.data_vars),\n",
    "#                     'dtype': str(dtypes.pop())})\n",
    "#     profile.update(profile_override)\n",
    "#     with rasterio.open(filename, 'w', **profile) as dest:\n",
    "#         for bandnum, data in enumerate(dataset.data_vars.values(), start=1):\n",
    "#             dest.write(data.data, bandnum)\n",
    "\n",
    "            \n",
    "def pq_fuser(dest, src):\n",
    "    \n",
    "    \"\"\"Fuse datacube PQ data\"\"\"\n",
    "    \n",
    "    valid_val = (1 << valid_bit)\n",
    "\n",
    "    no_data_dest_mask = ~(dest & valid_val).astype(bool)\n",
    "    np.copyto(dest, src, where=no_data_dest_mask)\n",
    "\n",
    "    both_data_mask = (valid_val & dest & src).astype(bool)\n",
    "    np.copyto(dest, src & dest, where=both_data_mask)\n",
    "\n",
    "    \n",
    "def hltc_import(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports high and low composite data for a given spatial query, and\n",
    "    return an array and projection information\n",
    "    :attr query: spatial query for datacube.load()\n",
    "    :returns: 'row x col x band' array, geo transform and projection string\n",
    "    \"\"\"\n",
    "\n",
    "    # Import data\n",
    "    low_tide = dc.load(product = 'low_tide_comp_20p', **query)\n",
    "    high_tide = dc.load(product = 'high_tide_comp_20p', **query)\n",
    "\n",
    "    # Extract transform/projection from xarray dataset\n",
    "    geo_transform = low_tide.geobox.transform.to_gdal()\n",
    "    proj = low_tide.geobox.crs.wkt\n",
    "\n",
    "    # Rename variables in each high/low composite so datasets can be merged\n",
    "    data_vars = list(low_tide.keys())[3:]  # select only data vars, not coords\n",
    "    low_tide.rename({var: \"lt_\" + var for var in data_vars}, inplace = True)\n",
    "    high_tide.rename({var: \"ht_\" + var for var in data_vars}, inplace = True)\n",
    "\n",
    "    # Combine into one dataset\n",
    "    raster_input = xr.auto_combine([low_tide, high_tide]).isel(time = 0)\n",
    "\n",
    "    # Convert to array; reorder dimensions\n",
    "    bands_array = raster_input.to_array().values\n",
    "    bands_array = np.einsum('bxy->xyb', bands_array)\n",
    "    \n",
    "    return bands_array, geo_transform, proj\n",
    "\n",
    "\n",
    "def point_in_poly(lon, lat, field, shapefile):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract data from polygon that contains point\n",
    "    :attr lon: x coordinate of point\n",
    "    :attr lat: y coordinate of point\n",
    "    :attr field: field of polygon shapefile to extract for matching points\n",
    "    :attr shapefile: polygon shapefile to import\n",
    "    :returns: if point falls within polygon, return field from polygon\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert coordinates to shapely point\n",
    "    mypoint = Point(lon, lat)   \n",
    "\n",
    "    # Extract polygon info\n",
    "    with fiona.open(shapefile) as shp:\n",
    "\n",
    "        # For each polygon, identify if point falls within polygon\n",
    "        poly_idx = [poly['properties']['ID'] for i, poly in enumerate(shp)\n",
    "                    if mypoint.within(shape(poly['geometry']))]\n",
    "\n",
    "    # If point is within polygon\n",
    "    if poly_idx:\n",
    "        \n",
    "        # Take first polygon to avoid multiple matches        \n",
    "        return(poly_idx[0])\n",
    "\n",
    "    # If point not found in any polygon, return none\n",
    "    else: \n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "def layer_extent(layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes min and max extents for GDAL layer features. Compared to\n",
    "    built-in \".GetExtent\" that always returns unfiltered extents, this \n",
    "    allows you to compute extents of features within filtered layers \n",
    "    (e.g. layers filtered with 'SetAttributeFilter')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract tuples of x, y, z coordinates for each point feature\n",
    "    point_coords = [feature.geometry().GetPoint() for feature in layer]\n",
    "    \n",
    "    # Compute mins and maxes across points for each tuple element\n",
    "    max_x, max_y, max_z = map(min, zip(*point_coords))\n",
    "    min_x, min_y, min_z = map(max, zip(*point_coords))    \n",
    "    \n",
    "    return  min_x, max_x, min_y, max_y\n",
    "\n",
    "\n",
    "# Set up paths\n",
    "classification_output = \"output_data/classification_dc_hltc_mangroves.tiff\"\n",
    "train_data_path = \"raw_data/train\"\n",
    "# validation_data_path = \"raw_data/test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import HLTC training data and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data for polygon 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/_collections_abc.py:702: FutureWarning: calling len() on an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Call len() on the Dataset.variables property instead, like ``len(ds.variables)``, to preserve existing behavior in a forwards compatible manner.\n",
      "  return len(self._mapping)\n",
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/_collections_abc.py:720: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  yield from self._mapping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data for polygon 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3/1.5.4/lib/python3.6/site-packages/datacube/utils/__init__.py:180: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  res = (data[data.size - 1] - data[0]) / (data.size - 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data for polygon 106\n",
      "Extracting training data for polygon 183\n",
      "Extracting training data for polygon 157\n",
      "\n",
      "Model trained on 281 training samples\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each point and extract matching polygon data\n",
    "with fiona.open(train_data_path + \"/training_data_mangrove.shp\") as points:\n",
    "        \n",
    "        # Output IDs\n",
    "        poly_ids = list()\n",
    "        \n",
    "        # For each point, identify what polygon it belongs to        \n",
    "        for point in points:\n",
    "            \n",
    "            # Convert albers coordinates to WGS84    \n",
    "            lon, lat = point['geometry']['coordinates']        \n",
    "            x, y = geometry.point(lon, lat, CRS('EPSG:3577')).to_crs(CRS('WGS84')).points[0]\n",
    "            \n",
    "            poly_id = point_in_poly(lon=x, lat=y,\n",
    "                                    field='ID',\n",
    "                                    shapefile='/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "            \n",
    "            poly_ids.append(poly_id)\n",
    "\n",
    "# Open vector of training points with gdal\n",
    "data_source = gdal.OpenEx(train_data_path + \"/training_data_mangrove.shp\", gdal.OF_VECTOR)\n",
    "layer = data_source.GetLayer(0)\n",
    "\n",
    "# Output training label and pixel arrays\n",
    "training_labels_list = list()\n",
    "training_samples_list = list()\n",
    "\n",
    "# For each polygon, extract datacube data using extent of matching points\n",
    "# and add resulting spectral data and labels to list of arrays\n",
    "for polygon_id in set(poly_ids):\n",
    "    \n",
    "    print(\"Extracting training data for polygon {}\".format(polygon_id))\n",
    "\n",
    "    # List of matching FIDs\n",
    "    fid_list = [i for i, x in enumerate(poly_ids) if x == polygon_id]    \n",
    "\n",
    "    # Temporary fix for polygons containing only one training point: setAttributeFilter\n",
    "    # returns all polygons if passed a single element tuple, so duplicate the single FID\n",
    "    if len(fid_list) == 1:  \n",
    "        fid_list = [fid_list[0], fid_list[0]]     \n",
    "        \n",
    "    # Filter layer by passing list of FIDs to SQL query as tuple, i.e. \"(1, 2, ...)\"\n",
    "    layer.SetAttributeFilter(\"FID IN {}\".format(tuple(fid_list)))\n",
    "        \n",
    "        \n",
    "    # Compute extents and generate spatial query\n",
    "    xmin, xmax, ymin, ymax = layer_extent(layer) \n",
    "    query_train = {'x': (xmin, xmax),\n",
    "                   'y': (ymin, ymax),\n",
    "                   'crs': 'EPSG:3577'}\n",
    "\n",
    "    # Import data \n",
    "    bands_array_train, geo_transform_train, proj_train = hltc_import(query_train)\n",
    "    rows_train, cols_train, bands_n_train = bands_array_train.shape\n",
    "\n",
    "    # Import training data shapefiles and convert to matching raster pixels\n",
    "    training_shapefile = train_data_path + \"/training_data_mangrove.shp\"\n",
    "    training_pixels = rasterize_vector(layer, cols_train, rows_train, \n",
    "                                       geo_transform_train, proj_train, field = \"class\")  \n",
    "    \n",
    "    # Extract matching image sample data for each labelled pixel location\n",
    "    is_train = np.nonzero(training_pixels)\n",
    "    training_labels = training_pixels[is_train]\n",
    "    training_samples = bands_array_train[is_train]\n",
    "\n",
    "    # Remove nans from training samples\n",
    "    training_labels = training_labels[~np.isnan(training_samples).any(axis=1)]\n",
    "    training_samples = training_samples[~np.isnan(training_samples).any(axis=1)]\n",
    "    \n",
    "    # Append outputs\n",
    "    training_labels_list.append(training_labels)\n",
    "    training_samples_list.append(training_samples)\n",
    "\n",
    "# Combine polygon training data    \n",
    "training_labels = np.concatenate(training_labels_list, axis=0)\n",
    "training_samples = np.concatenate(training_samples_list, axis=0)  \n",
    "\n",
    "# Set up classifier and train on training sample data and labels\n",
    "classifier = RandomForestClassifier(n_jobs = 4, n_estimators = 10)\n",
    "classifier.fit(training_samples, training_labels)\n",
    "print(\"\\nModel trained on {} training samples\".format(str(len(training_samples))))\n",
    "\n",
    "# Plot output random forest trees to file\n",
    "data_vars = [type + var for type in [\"lt_\", \"ht_\"] \n",
    "             for var in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']]\n",
    "\n",
    "# # For each tree in forest\n",
    "# for n, tree_in_forest in enumerate(classifier.estimators_):\n",
    "\n",
    "#     # Create graph and save to dot file\n",
    "#     export_graphviz(tree_in_forest,\n",
    "#                     out_file=\"figures/tree_graphs/tree.dot\",\n",
    "#                     feature_names = data_vars,\n",
    "#                     class_names = [\"mangrove\", \"water\", \"veg\", \"other\"],\n",
    "#                     filled=True,\n",
    "#                     rounded=True)\n",
    "\n",
    "#     # Plot as figure\n",
    "#     os.system('dot -Tpng figures/tree_graphs/tree.dot -o ' + \\\n",
    "#               'figures/tree_graphs/tree' + str(n + 1) + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import HLTC analysis data and prepare for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/_collections_abc.py:702: FutureWarning: calling len() on an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Call len() on the Dataset.variables property instead, like ``len(ds.variables)``, to preserve existing behavior in a forwards compatible manner.\n",
      "  return len(self._mapping)\n",
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/_collections_abc.py:720: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  yield from self._mapping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to classify:\n",
      "Rows: 2401\n",
      "Columns: 2401\n",
      "Bands: 12\n"
     ]
    }
   ],
   "source": [
    "# Set up analysis data query\n",
    "lat_point, lon_point, buffer = -12.5507304443, 130.802900172, 30000\n",
    "x, y = geometry.point(lon_point, lat_point, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Import analysis data\n",
    "bands_array, geo_transform, proj = hltc_import(query)\n",
    "rows, cols, bands_n = bands_array.shape\n",
    "print(\"Data to classify:\\nRows: {0}\\nColumns: {1}\\nBands: {2}\".format(rows, cols, bands_n))\n",
    "\n",
    "# Remove nodata and return flattened 'pixel x bands' array\n",
    "input_nodata = np.isnan(bands_array).any(axis = 2)\n",
    "flat_pixels = bands_array[~input_nodata]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification running...\n",
      "  Classification complete\n",
      "  0 nodata cells removed\n",
      "    Classification exported\n"
     ]
    }
   ],
   "source": [
    "# Run classification\n",
    "print(\"Classification running...\")\n",
    "result = classifier.predict(flat_pixels)\n",
    "\n",
    "# Restore 2D array by assigning flattened output to empty array\n",
    "classification = np.zeros((rows, cols))\n",
    "classification[~input_nodata] = result\n",
    "print(\"  Classification complete\")\n",
    "\n",
    "# Nodata removed\n",
    "print(\"  \" + str(np.sum(classification == 0)) + \" nodata cells removed\")\n",
    "\n",
    "# Export to file\n",
    "write_geotiff(classification_output, \n",
    "              data = classification, \n",
    "              geo_transform = geo_transform, \n",
    "              projection = proj,\n",
    "              nodata_val = 0)\n",
    "print(\"    Classification exported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verification data\n",
    "# shapefiles = glob.glob(validation_data_path + \"/*.shp\")\n",
    "# classes = [i.split(\"/\")[2][0:1] for i in shapefiles]\n",
    "# verification_pixels = vectors_to_raster(shapefiles, rows, cols, geo_transform, proj)\n",
    "# for_verification = np.nonzero(verification_pixels)\n",
    "# verification_labels = verification_pixels[for_verification]\n",
    "# predicted_labels = classification[for_verification]\n",
    "\n",
    "# # Confusion matrix\n",
    "# print(\"Confussion matrix:\\n%s\" %\n",
    "#       metrics.confusion_matrix(verification_labels, predicted_labels))\n",
    "# target_names = ['Class %s' % s for s in classes]\n",
    "\n",
    "# # Per class report\n",
    "# print(\"Classification report:\\n%s\" %\n",
    "#       metrics.classification_report(verification_labels, predicted_labels,\n",
    "#                                     target_names=target_names))\n",
    "\n",
    "# # Overall classification accuracy\n",
    "# print(\"Classification accuracy: %f\" %\n",
    "#       metrics.accuracy_score(verification_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old import NBAR from datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ('2015-01-01', '2015-03-01'), 'x': (1901405.2630724623, 1961405.2630724623), 'y': (-3660643.2668699455, -3600643.2668699455), 'crs': 'EPSG:3577'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 7, x: 2401, y: 2401)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2015-01-11T23:43:21.500000 ...\n",
       "  * y        (y) float64 -3.601e+06 -3.601e+06 -3.601e+06 -3.601e+06 ...\n",
       "  * x        (x) float64 1.901e+06 1.901e+06 1.901e+06 1.901e+06 1.902e+06 ...\n",
       "Data variables:\n",
       "    blue     (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "    green    (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "    red      (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "    nir      (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "    swir1    (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "    swir2    (time, y, x) float64 nan nan nan nan nan nan nan nan nan nan ...\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define temporal range\n",
    "# start_of_epoch, end_of_epoch = '2015-01-01', '2015-03-01'\n",
    "\n",
    "# # Define wavelengths/bands of interest\n",
    "# bands_of_interest = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "\n",
    "\n",
    "# # Define sensor of interest\n",
    "# sensor1 = 'ls8'\n",
    "\n",
    "# # Location\n",
    "# lat_point, lon_point, buffer = -31.88, 152.69, 30000\n",
    "# x, y = geometry.point(lon_point, lat_point, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "\n",
    "# # Set up query\n",
    "# query = {'time': (start_of_epoch, end_of_epoch)}\n",
    "# query['x'] = (x - buffer, x + buffer)\n",
    "# query['y'] = (y - buffer, y + buffer)\n",
    "# query['crs'] = 'EPSG:3577'\n",
    "# print(query)\n",
    "\n",
    "# # Group PQ by solar day to avoid idiosyncracies of N/S overlap differences in PQ algorithm performance\n",
    "# pq_albers_product = dc.index.products.get_by_name(sensor1 + '_pq_albers')\n",
    "# valid_bit = pq_albers_product.measurements['pixelquality']['flags_definition']['contiguous']['bits']\n",
    "\n",
    "# # Load sensor specific band adjustment tuples for TSS \n",
    "# # ls5_tss_constant, ls5_tss_exponent = 3983, 1.6246\n",
    "# # ls7_tss_constant, ls7_tss_exponent = 3983, 1.6246\n",
    "# # ls8_tss_constant, ls8_tss_exponent = 3957, 1.6436   \n",
    "\n",
    "# # Retrieve the NBAR and PQ data for sensor n\n",
    "# sensor1_nbar = dc.load(product = sensor1 + '_nbar_albers', \n",
    "#                        group_by = 'solar_day', \n",
    "#                        measurements = bands_of_interest,  \n",
    "#                        **query)\n",
    "# sensor1_pq = dc.load(product = sensor1 + '_pq_albers', \n",
    "#                      group_by = 'solar_day', \n",
    "#                      fuse_func = pq_fuser, \n",
    "#                      **query)\n",
    "\n",
    "# # Extract projection information\n",
    "# crs = sensor1_nbar.crs.wkt\n",
    "# affine = sensor1_nbar.affine\n",
    "\n",
    "# # Ensure 1:1 match between NBAR and PQ\n",
    "# sensor1_nbar = sensor1_nbar.sel(time = sensor1_pq.time)\n",
    "\n",
    "# # Generate PQ masks and apply to remove cloud, cloud shadow and saturated observations\n",
    "# s1_cloud_free = masking.make_mask(sensor1_pq,\n",
    "#                                   cloud_acca = 'no_cloud',\n",
    "#                                   cloud_shadow_acca = 'no_cloud_shadow',\n",
    "#                                   cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "#                                   cloud_fmask = 'no_cloud',\n",
    "#                                   blue_saturated = False,\n",
    "#                                   green_saturated = False,\n",
    "#                                   red_saturated = False,\n",
    "#                                   nir_saturated = False,\n",
    "#                                   swir1_saturated = False,\n",
    "#                                   swir2_saturated = False,\n",
    "#                                   contiguous = True)\n",
    "\n",
    "# # Extract good data\n",
    "# s1_good_data = s1_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "# sensor1_nbar = sensor1_nbar.where(s1_good_data)\n",
    "# sensor1_nbar\n",
    "\n",
    "# # Single clear timestep 6 (time 2015-02-28T23:43:05)\n",
    "# raster_input = sensor1_nbar.isel(time = 6)\n",
    "# raster_input.red.plot()\n",
    "\n",
    "# # Write data to file\n",
    "# write_multibandgeotiff(filename=\"raw_data/input_raster.tif\", dataset=raster_input) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
