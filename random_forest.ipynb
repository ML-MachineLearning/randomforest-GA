{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load modules\n",
    "import datacube\n",
    "import fiona\n",
    "import glob\n",
    "import itertools\n",
    "import os\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_invalid_data\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import shape\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Set up datacube instance\n",
    "dc = datacube.Datacube(app = 'Random forest classification')\n",
    "\n",
    "\n",
    "def load_nbart(sensor, query):\n",
    "    \n",
    "    '''\n",
    "    Loads nbart data for a sensor, masks using pq, then filters out terrain -999s\n",
    "    function written 23-08-2017 based on dc v1.5.1\n",
    "    '''  \n",
    "    \n",
    "    product_name = '{}_{}_albers'.format(sensor, 'nbart')\n",
    "    print('Loading {}'.format(product_name))\n",
    "    ds = dc.load(product = product_name,\n",
    "                 group_by = 'solar_day', \n",
    "                 **query)\n",
    "    \n",
    "    if ds:       \n",
    "   \n",
    "        print('Loaded {}'.format(product_name))\n",
    "        \n",
    "        # Extract PQ data for masking\n",
    "        mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "        sensor_pq = dc.load(product = mask_product, \n",
    "                            fuse_func = ga_pq_fuser,\n",
    "                            group_by = 'solar_day', \n",
    "                            **query)\n",
    "        \n",
    "        if sensor_pq:\n",
    "            \n",
    "            print('Making mask {}'.format(mask_product))\n",
    "            cloud_free = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                           cloud_acca = 'no_cloud',\n",
    "                                           cloud_shadow_acca = 'no_cloud_shadow',                           \n",
    "                                           cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                                           cloud_fmask = 'no_cloud',\n",
    "                                           blue_saturated = False,\n",
    "                                           green_saturated = False,\n",
    "                                           red_saturated = False,\n",
    "                                           nir_saturated = False,\n",
    "                                           swir1_saturated = False,\n",
    "                                           swir2_saturated = False,\n",
    "                                           contiguous = True)\n",
    "            \n",
    "            # Filter to remove clouds and -999 terrain issues\n",
    "            ds = ds.where(cloud_free)\n",
    "            ds = ds.where(ds != -999.0)\n",
    "            \n",
    "            # Add projection attributes\n",
    "            ds.attrs['crs'] = ds.crs\n",
    "            ds.attrs['affine'] = ds.affine    \n",
    "            ds.attrs['geo_transform'] = ds.geobox.transform.to_gdal()\n",
    "            ds.attrs['proj'] = ds.geobox.crs.wkt\n",
    "            print('Masked {} with {} and filtered ' \\\n",
    "                  'terrain'.format(product_name, mask_product))\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            print('Did not mask {} with {}'.format(product_name, mask_product))\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print ('Did not load {}'.format(product_name)) \n",
    "\n",
    "    if len(ds) > 0:\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "def rasterize_vector(input_data, cols, rows, geo_transform,\n",
    "                     projection, field):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rasterize a vector file and return numpy array\n",
    "    :attr input_data: input shapefile path or preloaded GDAL/OGR layer\n",
    "    :attr cols: width of output array in columns\n",
    "    :attr rows: height of output array in rows\n",
    "    :attr geo_transform: geotransform for rasterization\n",
    "    :attr projection: projection for rasterization\n",
    "    :attr field: shapefile field to take values from\n",
    "    :returns: a 'row x col' array containg values from vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # If input data is a string, import as shapefile layer\n",
    "    if isinstance(input_data, str):    \n",
    "    \n",
    "        # Open vector with gdal\n",
    "        data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "        input_data = data_source.GetLayer(0)\n",
    "    \n",
    "    # Set up output raster\n",
    "    driver = gdal.GetDriverByName('GTiff')  # 'MEM')  # In memory dataset\n",
    "    target_ds = driver.Create('test.tif', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    \n",
    "    # Rasterize shapefile and extract array\n",
    "    gdal.RasterizeLayer(target_ds, [1], input_data, options=[\"ATTRIBUTE=\" + field])\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    out_array = band.ReadAsArray()\n",
    "    target_ds = None\n",
    "    \n",
    "    return out_array\n",
    "\n",
    "\n",
    "def write_geotiff(fname, data, geo_transform, projection, nodata_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a single band GeoTIFF file with data from array.\n",
    "    :attr fname: output file path\n",
    "    :attr data: input array\n",
    "    :attr geo_transform: geotransform for output raster\n",
    "    :attr projection: projection for output raster\n",
    "    :attr nodata_val: value to convert to nodata in output raster\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # Create raster of given size and projection\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    \n",
    "    # Write data to array and set nodata values\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    band.SetNoDataValue(nodata_val)\n",
    "    \n",
    "    # Close file\n",
    "    dataset = None     \n",
    "    \n",
    "\n",
    "def tasseled_cap(sensor_data, sensor, tc_bands = ['greenness', 'brightness', 'wetness'], \n",
    "                 drop = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes tasseled cap wetness, greenness and brightness bands from a six\n",
    "    band xarray dataset, and returns a new xarray dataset with old bands \n",
    "    optionally dropped\n",
    "    :attr sensor_data: input xarray dataset with six Landsat bands\n",
    "    :attr tc_bands: list of tasseled cap bands to compute \n",
    "    (valid options: 'wetness', 'greenness','brightness'\n",
    "    :attr sensor: Landsat sensor used for coefficient values \n",
    "    (valid options: 'ls5', 'ls7', 'ls8')\n",
    "    :attr drop: if 'drop = False', return all original Landsat bands \n",
    "    :returns: xarray dataset with newly computed tasseled cap bands\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy input dataset\n",
    "    output_array = sensor_data.copy(deep = True)\n",
    " \n",
    "    # Coefficients for each tasseled cap band\n",
    "    wetness_coeff = {'ls5':{'blue':0.0315, 'green':0.2021, 'red':0.3102, \n",
    "                            'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109},\n",
    "                     'ls7':{'blue':0.0315, 'green':0.2021, 'red':0.3102, \n",
    "                            'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109},\n",
    "                     'ls8':{'blue':0.0315, 'green':0.2021, 'red':0.3102, \n",
    "                            'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109}}\n",
    "    \n",
    "    greenness_coeff = {'ls5':{'blue':-0.1603, 'green':-0.2819, 'red':-0.4934, \n",
    "                              'nir':0.7940, 'swir1':-0.0002, 'swir2':-0.1446},\n",
    "                       'ls7':{'blue':-0.1603, 'green':-0.2819, 'red':-0.4934, \n",
    "                              'nir':0.7940, 'swir1':-0.0002, 'swir2':-0.1446},\n",
    "                       'ls8':{'blue':-0.1603, 'green':-0.2819, 'red':-0.4934, \n",
    "                              'nir':0.7940, 'swir1':-0.0002, 'swir2':-0.1446}}\n",
    "    \n",
    "    brightness_coeff = {'ls5':{'blue':0.2043, 'green':0.4158, 'red':0.5524, \n",
    "                               'nir':0.5741, 'swir1':0.3124, 'swir2':0.2303},\n",
    "                        'ls7':{'blue':0.2043, 'green':0.4158, 'red':0.5524, \n",
    "                               'nir':0.5741, 'swir1':0.3124, 'swir2':0.2303},\n",
    "                        'ls8':{'blue':0.2043, 'green':0.4158, 'red':0.5524, \n",
    "                               'nir':0.5741, 'swir1':0.3124, 'swir2':0.2303}}\n",
    "    \n",
    "    # Dict to use correct coefficients for each tasseled cap band\n",
    "    analysis_coefficient = {'wetness': wetness_coeff, \n",
    "                            'greenness': greenness_coeff,\n",
    "                            'brightness': brightness_coeff}\n",
    "    \n",
    "    # For each band, compute tasseled cap band and add to output dataset\n",
    "    for tc_band in tc_bands:\n",
    "\n",
    "        # Create xarray of coefficient values used to multiply each band of input\n",
    "        coeff = xr.Dataset(analysis_coefficient[tc_band][sensor])    \n",
    "        sensor_coeff = sensor_data * coeff\n",
    "\n",
    "        # Sum all bands\n",
    "        output_array[tc_band] = sensor_coeff.blue + sensor_coeff.green + \\\n",
    "                                sensor_coeff.red + sensor_coeff.nir + \\\n",
    "                                sensor_coeff.swir1 + sensor_coeff.swir2\n",
    "    \n",
    "    # If drop = True, remove original bands\n",
    "    if drop:\n",
    "        \n",
    "        bands_to_drop = list(sensor_data.data_vars)        \n",
    "        output_array = output_array.drop(bands_to_drop)        \n",
    "\n",
    "    return(output_array)\n",
    "\n",
    "\n",
    "def point_in_poly(lon, lat, field, shapefile):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract data from polygon that contains point\n",
    "    :attr lon: x coordinate of point\n",
    "    :attr lat: y coordinate of point\n",
    "    :attr field: field of polygon shapefile to extract for matching points\n",
    "    :attr shapefile: polygon shapefile to import\n",
    "    :returns: if point falls within polygon, return field from polygon\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert coordinates to shapely point\n",
    "    mypoint = Point(lon, lat)   \n",
    "\n",
    "    # Extract polygon info\n",
    "    with fiona.open(shapefile) as shp:\n",
    "\n",
    "        # For each polygon, identify if point falls within polygon\n",
    "        poly_idx = [poly['properties'][field] for i, poly in enumerate(shp)\n",
    "                    if mypoint.within(shape(poly['geometry']))]\n",
    "\n",
    "    # If point is within polygon\n",
    "    if poly_idx:\n",
    "        \n",
    "        # Take first polygon to avoid multiple matches        \n",
    "        return(poly_idx[0])\n",
    "\n",
    "    # If point not found in any polygon, return none\n",
    "    else: \n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "def layer_extent(layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes min and max extents for GDAL layer features. Compared to\n",
    "    built-in \".GetExtent\" that always returns unfiltered extents, this \n",
    "    allows you to compute extents of features within filtered layers \n",
    "    (e.g. layers filtered with 'SetAttributeFilter')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract tuples of x, y, z coordinates for each point feature\n",
    "    point_coords = [feature.geometry().GetPoint() for feature in layer]\n",
    "    \n",
    "    # Compute mins and maxes across points for each tuple element\n",
    "    max_x, max_y, max_z = map(min, zip(*point_coords))\n",
    "    min_x, min_y, min_z = map(max, zip(*point_coords))    \n",
    "    \n",
    "    return  min_x, max_x, min_y, max_y\n",
    "\n",
    "\n",
    "def randomforest_train(train_shps, train_field, data_func, \n",
    "                       data_func_params = {}, classifier_params = {}):\n",
    "    \n",
    "    '''\n",
    "    Extracts training data from xarray dataset for multiple training shapefiles\n",
    "    '''\n",
    "\n",
    "    # Output training label and pixel arrays\n",
    "    training_labels_list = list()\n",
    "    training_samples_list = list()\n",
    "\n",
    "    # For each shapefile, extract datacube data using extent of points\n",
    "    # and add resulting spectral data and labels to list of arrays\n",
    "    for train_shp in train_shps:\n",
    "\n",
    "        print(\"Importing training data from {}\".format(train_shp))\n",
    "\n",
    "        # Open vector of training points with gdal\n",
    "        data_source = gdal.OpenEx(train_shp, gdal.OF_VECTOR)\n",
    "        layer = data_source.GetLayer(0)      \n",
    "\n",
    "        # Compute extents and generate spatial query\n",
    "        xmin, xmax, ymin, ymax = layer_extent(layer) \n",
    "        query_train = {'x': (xmin + 500, xmax - 500),\n",
    "                       'y': (ymin + 500, ymax - 500),\n",
    "                       'crs': 'EPSG:3577',\n",
    "                       **data_func_params}        \n",
    "        print(query_train)\n",
    "\n",
    "        # Import data  as xarray and extract projection/transform data\n",
    "        training_xarray = data_func(query_train)\n",
    "        geo_transform_train = training_xarray.geo_transform\n",
    "        proj_train = training_xarray.proj \n",
    "\n",
    "        # Covert to array and rearrange dimension order\n",
    "        bands_array_train = training_xarray.to_array().values\n",
    "        bands_array_train = np.einsum('bxy->xyb', bands_array_train)\n",
    "        rows_train, cols_train, bands_n_train = bands_array_train.shape\n",
    "\n",
    "        # Import training data shapefiles and convert to matching raster pixels\n",
    "        training_shapefile = train_shp\n",
    "        training_pixels = rasterize_vector(layer, cols_train, rows_train, \n",
    "                                           geo_transform_train, proj_train, \n",
    "                                           field = train_field)  \n",
    "\n",
    "        # Extract matching image sample data for each labelled pixel location\n",
    "        is_train = np.nonzero(training_pixels)\n",
    "        training_labels = training_pixels[is_train]\n",
    "        training_samples = bands_array_train[is_train]\n",
    "\n",
    "        # Remove nans from training samples\n",
    "        training_labels = training_labels[~np.isnan(training_samples).any(axis=1)]\n",
    "        training_samples = training_samples[~np.isnan(training_samples).any(axis=1)]\n",
    "\n",
    "        # Append outputs\n",
    "        training_labels_list.append(training_labels)\n",
    "        training_samples_list.append(training_samples)\n",
    "\n",
    "    # Combine polygon training data    \n",
    "    training_labels = np.concatenate(training_labels_list, axis=0)\n",
    "    training_samples = np.concatenate(training_samples_list, axis=0)  \n",
    "\n",
    "    # Set up classifier and train on training sample data and labels\n",
    "    # Options for tuning: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "    print(\"\\nTraining random forest classifier...\")\n",
    "    classifier = RandomForestClassifier(**classifier_params) \n",
    "    classifier.fit(training_samples, training_labels)\n",
    "    print(\"Model trained on {0} bands and \"\n",
    "          \"{1} training samples\".format(training_samples.shape[1],\n",
    "                                        str(len(training_samples))))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "def randomforest_classify(classifier, analysis_data, classification_output):\n",
    "    \n",
    "    '''\n",
    "    Performs classification using xarray dataset and classifier\n",
    "    '''\n",
    "\n",
    "    geo_transform = analysis_data.geo_transform\n",
    "    proj = analysis_data.proj \n",
    "\n",
    "    # Covert to array and rearrange dimension order\n",
    "    analysis_array = analysis_data.to_array().values\n",
    "    analysis_array = np.einsum('bxy->xyb', analysis_array)\n",
    "    rows, cols, bands_n = analysis_array.shape\n",
    "    print(\"Data to classify:\\nRows: {0}\\nColumns: {1}\\nBands: {2}\".format(rows, cols, bands_n))\n",
    "\n",
    "    # Remove nodata and return flattened 'pixel x bands' array\n",
    "    input_nodata = np.isnan(analysis_array).any(axis = 2)\n",
    "    flat_pixels = analysis_array[~input_nodata]\n",
    "\n",
    "    # Run classification\n",
    "    print(\"Classification running...\")\n",
    "    result = classifier.predict(flat_pixels)\n",
    "\n",
    "    # Restore 2D array by assigning flattened output to empty array\n",
    "    classification = np.zeros((rows, cols))\n",
    "    classification[~input_nodata] = result\n",
    "    print(\"  Classification complete\")\n",
    "\n",
    "    # Nodata removed\n",
    "    print(\"  \" + str(np.sum(classification == 0)) + \" nodata cells removed\")\n",
    "\n",
    "    # Export to file\n",
    "    write_geotiff(classification_output, \n",
    "                  data = classification, \n",
    "                  geo_transform = geo_transform, \n",
    "                  projection = proj,\n",
    "                  nodata_val = 0)\n",
    "    print(\"    Classification exported\")\n",
    "\n",
    "\n",
    "# Set up paths\n",
    "classification_output = \"output_data/classification_dc_hltc_mangroves.tiff\"\n",
    "train_data_path = \"raw_data/train\"\n",
    "# validation_data_path = \"raw_data/test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with TC indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hltc_import(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports high and low composite data for a given spatial query, and\n",
    "    return an xarray dataset with projection and geo transform attributes\n",
    "    :attr query: spatial query for datacube.load()\n",
    "    :returns: xarray dataset with geo transform and projection attributes\n",
    "    \"\"\"\n",
    "\n",
    "    # Import data\n",
    "    low_tide = dc.load(product = 'low_tide_comp_20p', **query)\n",
    "    high_tide = dc.load(product = 'high_tide_comp_20p', **query)\n",
    "\n",
    "    # Rename variables in each high/low composite so datasets can be merged\n",
    "    data_vars = list(low_tide.keys())[3:]  # select only data vars, not coords\n",
    "    low_tide.rename({var: \"lt_\" + var for var in data_vars}, inplace = True)\n",
    "    high_tide.rename({var: \"ht_\" + var for var in data_vars}, inplace = True)\n",
    "\n",
    "    # Combine into one dataset\n",
    "    output_xarray = xr.auto_combine([low_tide, high_tide]).isel(time = 0)\n",
    "    \n",
    "    # Set attributes   \n",
    "    output_xarray.attrs['proj'] = low_tide.geobox.crs.wkt\n",
    "    output_xarray.attrs['geo_transform'] = low_tide.geobox.transform.to_gdal()    \n",
    "    \n",
    "    return output_xarray\n",
    "\n",
    "\n",
    "# Test function for importing tasseled cap data\n",
    "def tc_import(query):\n",
    "    \n",
    "    '''\n",
    "    Wrapper around load_nbart and tasseled cap to generate data for \n",
    "    classifier training and analysis.\n",
    "    :attr query: query for datacube call; for training, supply only\n",
    "    non-spatial queries as spatial are generated from training data\n",
    "    :returns: xarray dataset with geo transform and projection attributes\n",
    "    '''\n",
    "    \n",
    "    nbar_example = load_nbart('ls8', query)\n",
    "        \n",
    "    training_xarray = tasseled_cap(sensor_data = nbar_example, \n",
    "                                   sensor = 'ls8',\n",
    "                                   drop = True).median(\"time\", keep_attrs = True)\n",
    "    \n",
    "    return training_xarray\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing training data from raw_data/train/training_data_tasseledcap.shp\n",
      "{'x': (352998.0302334339, 313256.0191187404), 'y': (-1317049.5107177817, -1347981.8454887671), 'crs': 'EPSG:3577', 'time': ('2017-03-01', '2017-06-28')}\n",
      "Loading ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:43: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:54: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making mask ls8_pq_albers\n",
      "Masked ls8_nbart_albers with ls8_pq_albers and filtered terrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:89: FutureWarning: calling len() on an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Call len() on the Dataset.variables property instead, like ``len(ds.variables)``, to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training random forest classifier...\n",
      "Model trained on 3 bands and 71 training samples\n"
     ]
    }
   ],
   "source": [
    "# List of training files to import. Each file should cover a small enough spatial area \n",
    "# so as to not slow dc.load function excessively (e.g. 100 x 100km max)\n",
    "train_shps = [\"raw_data/train/training_data_tasseledcap.shp\"]\n",
    "\n",
    "classifier_params = {'n_jobs': -1,                                    \n",
    "                     'n_estimators': 100,\n",
    "                     'max_features': \"auto\",\n",
    "                     'min_samples_leaf': 3,\n",
    "                     'oob_score': True }\n",
    "\n",
    "data_func_params = {'time': ('2017-03-01', '2017-06-28')}\n",
    "\n",
    "classifier = randomforest_train(train_shps = train_shps,\n",
    "                                train_field = \"class\",\n",
    "                                data_func = tc_import, #   hltc_import,\n",
    "                                data_func_params = data_func_params,\n",
    "                                classifier_params = classifier_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import analysis data and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to classify:\n",
      "Rows: 1601\n",
      "Columns: 1601\n",
      "Bands: 3\n",
      "Classification running...\n",
      "  Classification complete\n",
      "  0 nodata cells removed\n",
      "    Classification exported\n"
     ]
    }
   ],
   "source": [
    "# Set up analysis data query\n",
    "lat_point, lon_point, buffer = -12.5615704994, 135.013735867, 20000\n",
    "x, y = geometry.point(lon_point, lat_point, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': ('2017-03-01', '2017-06-28'),\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Load data from datacube\n",
    "analysis_xarray = tc_import(query)\n",
    "# analysis_xarray = hltc_import(query)\n",
    "\n",
    "# Run classification and export to file   \n",
    "randomforest_classify(classifier = classifier,\n",
    "                      analysis_data = analysis_xarray,\n",
    "                      classification_output = \"output_data/classification_dc_tasseledcap3.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Plot output random forest trees to file\n",
    "# for n, tree_in_forest in enumerate(classifier.estimators_):\n",
    "\n",
    "#     # Create graph and save to dot file\n",
    "#     export_graphviz(tree_in_forest,\n",
    "#                     out_file=\"figures/tree_graphs/tree.dot\",\n",
    "#                     feature_names = list(training_xarray.keys())[3:],\n",
    "#                     class_names = [\"mangrove\", \"water\", \"veg\", \"other\"],\n",
    "#                     filled=True,\n",
    "#                     rounded=True)\n",
    "\n",
    "#     # Plot as figure\n",
    "#     os.system('dot -Tpng figures/tree_graphs/tree.dot -o ' + \\\n",
    "#               'figures/tree_graphs/tree' + str(n + 1) + '.png')\n",
    "\n",
    "# # NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# # support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# # error trajectory during training.\n",
    "\n",
    "# # Test scenarios\n",
    "# ensemble_clfs = [(\"max_features='sqrt'\",\n",
    "#                   RandomForestClassifier(warm_start=True, oob_score=True,\n",
    "#                                          max_features=\"sqrt\")),\n",
    "#                  (\"max_features='0.5'\",\n",
    "#                   RandomForestClassifier(warm_start=True, max_features=0.5,\n",
    "#                                          oob_score=True)),\n",
    "#                  (\"max_features=None\",\n",
    "#                   RandomForestClassifier(warm_start=True, max_features=None,\n",
    "#                                          oob_score=True))]\n",
    "\n",
    "# ensemble_clfs = [(\"Leaf = 1\",\n",
    "#                   RandomForestClassifier(warm_start=True, \n",
    "#                                          min_samples_leaf = 1,\n",
    "#                                          oob_score=True)),\n",
    "#                  (\"Leaf = 5\",\n",
    "#                   RandomForestClassifier(warm_start=True, \n",
    "#                                          min_samples_leaf = 5,\n",
    "#                                          oob_score=True)),\n",
    "#                  (\"Leaf = 20\",\n",
    "#                   RandomForestClassifier(warm_start=True, \n",
    "#                                          min_samples_leaf = 20,\n",
    "#                                          oob_score=True))]\n",
    "\n",
    "# # Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "# error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# # Range of `n_estimators` values to explore.\n",
    "# min_estimators = 1\n",
    "# max_estimators = 100\n",
    "\n",
    "# for label, clf in ensemble_clfs:\n",
    "#     for i in range(min_estimators, max_estimators + 1):\n",
    "#         clf.set_params(n_estimators=i)\n",
    "#         clf.fit(training_samples, training_labels)\n",
    "\n",
    "#         # Record the OOB error for each `n_estimators=i` setting.\n",
    "#         oob_error = 1 - clf.oob_score_\n",
    "#         error_rate[label].append((i, oob_error))\n",
    "\n",
    "# # Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "# for label, clf_err in error_rate.items():\n",
    "#     xs, ys = zip(*clf_err)\n",
    "#     plt.plot(xs, ys, label=label)\n",
    "\n",
    "# plt.xlim(min_estimators, max_estimators)\n",
    "# plt.xlabel(\"n_estimators\")\n",
    "# plt.ylabel(\"OOB error rate\")\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verification data\n",
    "# shapefiles = glob.glob(validation_data_path + \"/*.shp\")\n",
    "# classes = [i.split(\"/\")[2][0:1] for i in shapefiles]\n",
    "# verification_pixels = vectors_to_raster(shapefiles, rows, cols, geo_transform, proj)\n",
    "# for_verification = np.nonzero(verification_pixels)\n",
    "# verification_labels = verification_pixels[for_verification]\n",
    "# predicted_labels = classification[for_verification]\n",
    "\n",
    "# # Confusion matrix\n",
    "# print(\"Confussion matrix:\\n%s\" %\n",
    "#       metrics.confusion_matrix(verification_labels, predicted_labels))\n",
    "# target_names = ['Class %s' % s for s in classes]\n",
    "\n",
    "# # Per class report\n",
    "# print(\"Classification report:\\n%s\" %\n",
    "#       metrics.classification_report(verification_labels, predicted_labels,\n",
    "#                                     target_names=target_names))\n",
    "\n",
    "# # Overall classification accuracy\n",
    "# print(\"Classification accuracy: %f\" %\n",
    "#       metrics.accuracy_score(verification_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old import NBAR from datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define temporal range\n",
    "# start_of_epoch, end_of_epoch = '2015-01-01', '2015-03-01'\n",
    "\n",
    "# # Define wavelengths/bands of interest\n",
    "# bands_of_interest = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "\n",
    "\n",
    "# # Define sensor of interest\n",
    "# sensor1 = 'ls8'\n",
    "\n",
    "# # Location\n",
    "# lat_point, lon_point, buffer = -31.88, 152.69, 30000\n",
    "# x, y = geometry.point(lon_point, lat_point, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "\n",
    "# # Set up query\n",
    "# query = {'time': (start_of_epoch, end_of_epoch)}\n",
    "# query['x'] = (x - buffer, x + buffer)\n",
    "# query['y'] = (y - buffer, y + buffer)\n",
    "# query['crs'] = 'EPSG:3577'\n",
    "# print(query)\n",
    "\n",
    "# # Group PQ by solar day to avoid idiosyncracies of N/S overlap differences in PQ algorithm performance\n",
    "# pq_albers_product = dc.index.products.get_by_name(sensor1 + '_pq_albers')\n",
    "# valid_bit = pq_albers_product.measurements['pixelquality']['flags_definition']['contiguous']['bits']\n",
    "\n",
    "# # Load sensor specific band adjustment tuples for TSS \n",
    "# # ls5_tss_constant, ls5_tss_exponent = 3983, 1.6246\n",
    "# # ls7_tss_constant, ls7_tss_exponent = 3983, 1.6246\n",
    "# # ls8_tss_constant, ls8_tss_exponent = 3957, 1.6436   \n",
    "\n",
    "# # Retrieve the NBAR and PQ data for sensor n\n",
    "# sensor1_nbar = dc.load(product = sensor1 + '_nbar_albers', \n",
    "#                        group_by = 'solar_day', \n",
    "#                        measurements = bands_of_interest,  \n",
    "#                        **query)\n",
    "# sensor1_pq = dc.load(product = sensor1 + '_pq_albers', \n",
    "#                      group_by = 'solar_day', \n",
    "#                      fuse_func = pq_fuser, \n",
    "#                      **query)\n",
    "\n",
    "# # Extract projection information\n",
    "# crs = sensor1_nbar.crs.wkt\n",
    "# affine = sensor1_nbar.affine\n",
    "\n",
    "# # Ensure 1:1 match between NBAR and PQ\n",
    "# sensor1_nbar = sensor1_nbar.sel(time = sensor1_pq.time)\n",
    "\n",
    "# # Generate PQ masks and apply to remove cloud, cloud shadow and saturated observations\n",
    "# s1_cloud_free = masking.make_mask(sensor1_pq,\n",
    "#                                   cloud_acca = 'no_cloud',\n",
    "#                                   cloud_shadow_acca = 'no_cloud_shadow',\n",
    "#                                   cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "#                                   cloud_fmask = 'no_cloud',\n",
    "#                                   blue_saturated = False,\n",
    "#                                   green_saturated = False,\n",
    "#                                   red_saturated = False,\n",
    "#                                   nir_saturated = False,\n",
    "#                                   swir1_saturated = False,\n",
    "#                                   swir2_saturated = False,\n",
    "#                                   contiguous = True)\n",
    "\n",
    "# # Extract good data\n",
    "# s1_good_data = s1_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "# sensor1_nbar = sensor1_nbar.where(s1_good_data)\n",
    "# sensor1_nbar\n",
    "\n",
    "# # Single clear timestep 6 (time 2015-02-28T23:43:05)\n",
    "# raster_input = sensor1_nbar.isel(time = 6)\n",
    "# raster_input.red.plot()\n",
    "\n",
    "# # Write data to file\n",
    "# write_multibandgeotiff(filename=\"raw_data/input_raster.tif\", dataset=raster_input) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
